units = 32,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_activation_selu() %>%
layer_dense_variational(
units = 2,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_distribution_lambda(function(x)
tfd_normal(loc = x[, 1, drop = FALSE],
scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])
)
)
negloglik <- function(y, model) - (model %>% tfd_log_prob(y))
model %>% compile(optimizer = optimizer_adam(lr = 0.01), loss = negloglik)
model %>% fit(x, y, epochs = 200)
# each time we ask the model to predict, we get a different line
yhats <- purrr::map(1:100, function(x) model(tf$constant(x_test)))
means <-
purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()
sds <-
purrr::map(yhats, purrr::compose(as.matrix, tfd_stddev)) %>% abind::abind()
means_gathered <- data.frame(cbind(x_test, means)) %>%
gather(key = run, value = mean_val,-X1)
sds_gathered <- data.frame(cbind(x_test, sds)) %>%
gather(key = run, value = sd_val,-X1)
lines <-
means_gathered %>% inner_join(sds_gathered, by = c("X1", "run"))
mean <- apply(means, 1, mean)
ggplot(data.frame(x = x, y = y, mean = as.numeric(mean)), aes(x, y)) +
geom_point() +
theme(legend.position = "none") +
geom_line(aes(x = x_test, y = mean), color = "violet", size = 1.5) +
geom_line(
data = lines,
aes(x = X1, y = mean_val, color = run),
alpha = 0.6,
size = 0.5
) +
geom_ribbon(
data = lines,
aes(
x = X1,
ymin = mean_val - 2 * sd_val,
ymax = mean_val + 2 * sd_val,
group = run
),
alpha = 0.05,
fill = "grey",
inherit.aes = FALSE
)
model <- keras_model_sequential() %>%
layer_dense_variational(
units = 128,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 64,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 32,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 16,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 2,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_distribution_lambda(function(x)
tfd_normal(loc = x[, 1, drop = FALSE],
scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])
)
)
negloglik <- function(y, model) - (model %>% tfd_log_prob(y))
model %>% compile(optimizer = optimizer_adam(lr = 0.01), loss = negloglik)
model %>% fit(x, y, epochs = 200)
# each time we ask the model to predict, we get a different line
yhats <- purrr::map(1:100, function(x) model(tf$constant(x_test)))
means <-
purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()
sds <-
purrr::map(yhats, purrr::compose(as.matrix, tfd_stddev)) %>% abind::abind()
means_gathered <- data.frame(cbind(x_test, means)) %>%
gather(key = run, value = mean_val,-X1)
sds_gathered <- data.frame(cbind(x_test, sds)) %>%
gather(key = run, value = sd_val,-X1)
lines <-
means_gathered %>% inner_join(sds_gathered, by = c("X1", "run"))
mean <- apply(means, 1, mean)
ggplot(data.frame(x = x, y = y, mean = as.numeric(mean)), aes(x, y)) +
geom_point() +
theme(legend.position = "none") +
geom_line(aes(x = x_test, y = mean), color = "violet", size = 1.5) +
geom_line(
data = lines,
aes(x = X1, y = mean_val, color = run),
alpha = 0.6,
size = 0.5
) +
geom_ribbon(
data = lines,
aes(
x = X1,
ymin = mean_val - 2 * sd_val,
ymax = mean_val + 2 * sd_val,
group = run
),
alpha = 0.05,
fill = "grey",
inherit.aes = FALSE
)
ggplot(data.frame(x = x, y = y, mean = as.numeric(mean)), aes(x, y)) +
geom_point() +
theme(legend.position = "none") +
geom_line(aes(x = x_test, y = mean), color = "violet", size = 1.5) +
geom_line(
data = lines,
aes(x = X1, y = mean_val, color = run),
alpha = 0.6,
size = 0.5
) +
geom_ribbon(
data = lines,
aes(
x = X1,
ymin = mean_val - 2 * sd_val,
ymax = mean_val + 2 * sd_val,
group = run
),
alpha = 0.05,
fill = "grey",
inherit.aes = FALSE
)
library(tensorflow)
library(tfprobability)
library(keras)
library(MASS)
library(ggplot2)
library(data.table)
library(tidyr)
### Load data, and normalise predictors for ANN
data <- as.data.table(mcycle)
ggplot(data) + geom_point(aes(x = times, y = accel))
meantime <- mean(data$times)
sdtime <- sd(data$times)
datanorm <- copy(data)
datanorm[, times := (times-meantime)/sdtime, ]
ggplot(datanorm) + geom_point(aes(x = times, y = accel))
### Split data into train and test
testsamps <- sample(1:nrow(datanorm), nrow(datanorm)/10)
x_train <- as.matrix(datanorm[!testsamps, times])
y_train <- as.matrix(datanorm[!testsamps, accel])
x_test <- as.matrix(datanorm[testsamps, times])
y_test <- as.matrix(datanorm[testsamps, accel])
### Construct neural network
prior_trainable <-
function(kernel_size,
bias_size = 0,
dtype = NULL) {
n <- kernel_size + bias_size
keras_model_sequential() %>%
# we'll comment on this soon
# layer_variable(n, dtype = dtype, trainable = FALSE) %>%
layer_variable(n, dtype = dtype, trainable = TRUE) %>%
layer_distribution_lambda(function(t) {
tfd_independent(tfd_normal(loc = t, scale = 0.1),
reinterpreted_batch_ndims = 1)
})
}
posterior_mean_field <-
function(kernel_size,
bias_size = 0,
dtype = NULL) {
n <- kernel_size + bias_size
c <- log(expm1(1))
keras_model_sequential(list(
layer_variable(shape = 2 * n, dtype = dtype),
layer_distribution_lambda(
make_distribution_fn = function(t) {
tfd_independent(tfd_normal(
loc = t[1:n],
scale = 1e-5 + tf$nn$softplus(c + t[(n + 1):(2 * n)])
), reinterpreted_batch_ndims = 1)
}
)
))
}
# model <- keras_model_sequential() %>%
#   # layer_dense_variational(
#   #   units = 256, activation = "relu",
#   #   make_posterior_fn = posterior_mean_field,
#   #   make_prior_fn = prior_trainable,
#   #   kl_weight = 1 / nrow(x_train)
#   # ) %>%
#   layer_dense(
#     units = 256, activation = "relu") %>%
#   layer_distribution_lambda(function(x)
#     tfd_normal(loc = x, scale = 1))
model <- keras_model_sequential() %>%
layer_dense_variational(
units = 256,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
kl_weight = 1 / nrow(x_train)
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 128,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
kl_weight = 1 / nrow(x_train)
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 64,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
kl_weight = 1 / nrow(x_train)
) %>%
layer_activation_relu() %>%
layer_distribution_lambda(function(x)
tfd_normal(loc = x[, 1, drop = FALSE],
scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])
)
)
negloglik <- function(y, model) - (model %>% tfd_log_prob(y))
model %>% compile(optimizer = optimizer_adam(lr = 0.01), loss = negloglik)
### Train neural network
history <- model %>% fit(
x_train, y_train,
batch_size = 64,
epochs = 100,
verbose = 2,
validation_data = list(x_test, y_test),
callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 500, verbose = 1))
)
yhats <- purrr::map(1:100, function(x) model(tf$constant(x_test)))
means <-
purrr::map(yhats, purrr::compose(as.matrix, tfd_mean)) %>% abind::abind()
lines <- data.frame(cbind(x_test, means)) %>%
gather(key = run, value = value,-X1)
mean <- apply(means, 1, mean)
ggplot(data.frame(x = x_test, y = y_test, mean = as.numeric(mean)), aes(x, y)) +
geom_point() +
geom_line(aes(x = x_test, y = mean), color = "violet", size = 1.5) +
geom_line(
data = lines,
aes(x = X1, y = value, color = run),
alpha = 0.3,
size = 0.5
) +
theme(legend.position = "none")
library(tensorflow)
library(tfprobability)
library(keras)
library(MASS)
library(ggplot2)
library(data.table)
library(tidyr)
### Load data, and normalise predictors for ANN
data <- as.data.table(mcycle)
ggplot(data) + geom_point(aes(x = times, y = accel))
meantime <- mean(data$times)
sdtime <- sd(data$times)
datanorm <- copy(data)
datanorm[, times := (times-meantime)/sdtime, ]
ggplot(datanorm) + geom_point(aes(x = times, y = accel))
### Split data into train and test
testsamps <- sample(1:nrow(datanorm), nrow(datanorm)/10)
x_train <- as.matrix(datanorm[!testsamps, times])
y_train <- as.matrix(datanorm[!testsamps, accel])
x_test <- as.matrix(datanorm[testsamps, times])
y_test <- as.matrix(datanorm[testsamps, accel])
### Construct neural network
# scale the KL divergence by number of training examples
n <- nrow(x_train) %>% tf$cast(tf$float32)
kl_div <- function(q, p, unused)
tfd_kl_divergence(q, p) / n
model <- keras_model_sequential()
model %>%
layer_dense_flipout(
units = 48,
activation = "relu",
kernel_divergence_fn = kl_div
) %>%
layer_dense_flipout(
units = 2,
kernel_divergence_fn = kl_div,
name = "dense_output"
) %>%
layer_distribution_lambda(function(x)
tfd_normal(loc = x[, 1, drop = FALSE],
scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])
)
)
nll <- function(y, model) - (model %>% tfd_log_prob(y))
model %>% compile(
optimizer = "rmsprop",
loss = nll,
metrics = c("accuracy"),
experimental_run_tf_function = FALSE
)
train_history <- model %>% fit(
x_train,
epochs = 1000,
validation_data = list(x_test, y_test),
callbacks = list(
callback_early_stopping(patience = 10)
)
)
model <- keras_model_sequential()
model %>%
layer_dense_variational(
units = 48,
activation = "relu",
kernel_divergence_fn = kl_div
) %>%
layer_dense_variational(
units = 2,
kernel_divergence_fn = kl_div,
name = "dense_output"
) %>%
layer_distribution_lambda(function(x)
tfd_normal(loc = x[, 1, drop = FALSE],
scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])
)
)
nll <- function(y, model) - (model %>% tfd_log_prob(y))
model %>% compile(
optimizer = "rmsprop",
loss = nll,
metrics = c("accuracy"),
experimental_run_tf_function = FALSE
)
train_history <- model %>% fit(
x_train,
epochs = 1000,
validation_data = list(x_test, y_test),
callbacks = list(
callback_early_stopping(patience = 10)
)
)
n <- nrow(x_train) %>% tf$cast(tf$float32)
kl_div <- function(q, p, unused)
tfd_kl_divergence(q, p) / n
model <- keras_model_sequential()
model %>%
layer_dense_flipout(
units = 48,
activation = "relu",
kernel_divergence_fn = kl_div
) %>%
layer_dense_flipout(
units = 2,
kernel_divergence_fn = kl_div,
name = "dense_output"
) %>%
layer_distribution_lambda(function(x)
tfd_normal(loc = x[, 1, drop = FALSE],
scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])
)
)
nll <- function(y, model) - (model %>% tfd_log_prob(y))
model %>% compile(
optimizer = "rmsprop",
loss = nll,
metrics = c("accuracy"),
experimental_run_tf_function = FALSE
)
train_history <- model %>% fit(
x_train,
epochs = 1000,
validation_data = list(x_test, y_test),
callbacks = list(
callback_early_stopping(patience = 10)
)
)
library(tensorflow)
# assume it's version 1.14, with eager not yet being the default
tf$compat$v1$enable_v2_behavior()
library(tfprobability)
library(keras)
library(dplyr)
library(tidyr)
library(ggplot2)
# generate the data
x_min <- -40
x_max <- 60
n <- 1000
w0 <- 0.125
b0 <- 5
normalize <- function(x) (x - x_min) / (x_max - x_min)
# training data; predictor
x <- x_min + (x_max - x_min) * runif(n) %>% as.matrix()
# training data; target
eps <- rnorm(n) * (4 * (0.25 + (normalize(x)) ^ 2))^2
y <- (w0 * -1.6*x * (0.3 + 2*sin((x+40)/4) + b0)) + eps
# # training data; target
# eps <- rnorm(n) * (3 * (0.25 + (normalize(x)) ^ 2))
# y <- (w0 * x * (1 + sin(x)) + b0) + eps
# test data (predictor)
x_test <- seq(x_min, x_max, length.out = n) %>% as.matrix()
ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point()
prior_trainable <-
function(kernel_size,
bias_size,
dtype = NULL) {
n <- kernel_size + bias_size
keras_model_sequential() %>%
layer_variable(n, dtype = dtype, trainable = TRUE) %>%
layer_distribution_lambda(function(t) {
tfd_independent(tfd_normal(loc = t, scale = 1),
reinterpreted_batch_ndims = 1)
})
}
posterior_mean_field <-
function(kernel_size,
bias_size,
dtype = NULL) {
n <- kernel_size + bias_size
c <- log(expm1(1))
keras_model_sequential(list(
layer_variable(shape = 2 * n, dtype = dtype),
layer_distribution_lambda(
make_distribution_fn = function(t) {
tfd_independent(tfd_normal(
loc = t[1:n],
scale = 1e-5 + tf$nn$softplus(c + t[(n + 1):(2 * n)])
), reinterpreted_batch_ndims = 1)
}
)
))
}
model <- keras_model_sequential() %>%
layer_dense_variational(
units = 128,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 64,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 32,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 16,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_activation_relu() %>%
layer_dense_variational(
units = 2,
make_posterior_fn = posterior_mean_field,
make_prior_fn = prior_trainable,
# scale by the size of the dataset
kl_weight = 1 / n
) %>%
layer_distribution_lambda(function(x)
tfd_normal(loc = x[, 1, drop = FALSE],
scale = 1e-3 + tf$math$softplus(0.01 * x[, 2, drop = FALSE])
)
)
negloglik <- function(y, model) - (model %>% tfd_log_prob(y))
model %>% compile(optimizer = optimizer_adam(lr = 0.01), loss = negloglik)
model %>% fit(x, y, epochs = 200)
# each time we ask the model to predict, we get a different line
yhats <- purrr::map(1:100, function(x) model(tf$constant(x_test)))
