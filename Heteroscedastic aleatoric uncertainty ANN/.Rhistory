training = training
)
}
}
)
)
# function for instantiating custom wrapper
layer_concrete_dropout <- function(object,
layer,
weight_regularizer = 1e-6,
dropout_regularizer = 1e-5,
init_min = 0.1,
init_max = 0.1,
is_mc_dropout = TRUE,
name = NULL,
trainable = TRUE) {
create_wrapper(ConcreteDropout, object, list(
layer = layer,
weight_regularizer = weight_regularizer,
dropout_regularizer = dropout_regularizer,
init_min = init_min,
init_max = init_max,
is_mc_dropout = is_mc_dropout,
name = name,
trainable = trainable
))
}
input <- layer_input(shape = 1)
output <- input %>% layer_concrete_dropout(
layer_dense(units = 64, activation = 'relu', kernel_initializer = 'lecun_normal')
) %>% layer_concrete_dropout(
layer_dense(units = 32, activation = 'relu', kernel_initializer = 'lecun_normal')
)
mean <- output %>% layer_concrete_dropout(
layer = layer_dense(units = output_dim),
weight_regularizer = wd,
dropout_regularizer = dd
)
mean <- output %>% layer_concrete_dropout(
layer = layer_dense(units = 1),
weight_regularizer = wd,
dropout_regularizer = dd
)
# prior length-scale
l <- 1e-4
# initial value for weight regularizer
wd <- l^2/n_train
# initial value for dropout regularizer
dd <- 2/n_train
# sample size (training data)
n_train <- nrow(x_train)
# sample size (validation data)
n_val <- nrow(x_test)
# prior length-scale
l <- 1e-4
# initial value for weight regularizer
wd <- l^2/n_train
# initial value for dropout regularizer
dd <- 2/n_train
n_train <- nrow(x_train)
# sample size (validation data)
n_val <- nrow(x_test)
# prior length-scale
l <- 1e-4
# initial value for weight regularizer
wd <- l^2/n_train
# initial value for dropout regularizer
dd <- 2/n_train
input <- layer_input(shape = 1)
output <- input %>% layer_concrete_dropout(
layer_dense(units = 64, activation = 'relu', kernel_initializer = 'lecun_normal'),
weight_regularizer = wd,
dropout_regularizer = dd
) %>% layer_concrete_dropout(
layer_dense(units = 32, activation = 'relu', kernel_initializer = 'lecun_normal'),
weight_regularizer = wd,
dropout_regularizer = dd
)
mean <- output %>% layer_concrete_dropout(
layer = layer_dense(units = 1),
weight_regularizer = wd,
dropout_regularizer = dd
)
log_var <- output %>% layer_concrete_dropout(
layer_dense(units = output_dim),
weight_regularizer = wd,
dropout_regularizer = dd
)
input <- layer_input(shape = 1)
output <- input %>% layer_concrete_dropout(
layer_dense(units = 64, activation = 'relu', kernel_initializer = 'lecun_normal'),
weight_regularizer = wd,
dropout_regularizer = dd
) %>% layer_concrete_dropout(
layer_dense(units = 32, activation = 'relu', kernel_initializer = 'lecun_normal'),
weight_regularizer = wd,
dropout_regularizer = dd
)
mean <- output %>% layer_concrete_dropout(
layer = layer_dense(units = 1),
weight_regularizer = wd,
dropout_regularizer = dd
)
log_var <- output %>% layer_concrete_dropout(
layer_dense(units = 1),
weight_regularizer = wd,
dropout_regularizer = dd
)
output <- layer_concatenate(list(mean, log_var))
model <- keras_model(input, output)
heteroscedastic_loss <- function(y_true, y_pred) {
mean <- y_pred[, 1:1]
log_var <- y_pred[, (1 + 1):(1 * 2)]
precision <- k_exp(-log_var)
k_sum(precision * (y_true - mean) ^ 2 + log_var, axis = 2)
}
model %>% compile(
optimizer = "adam",
loss = heteroscedastic_loss,
metrics = c(custom_metric("heteroscedastic_loss", heteroscedastic_loss))
)
history <- model %>% fit(
x_train, y_train,
batch_size = 16,
epochs = 100,
verbose = 2,
validation_split = 0.1,
callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 100, verbose = 1),
callback_model_checkpoint(paste0(getwd(),"/mcycle_two_layer_mlp_relu.hdf5"), save_best_only = TRUE))
)
model <- load_model_hdf5(paste0(getwd(),"/mcycle_two_layer_mlp_relu.hdf5"))
model <- load_model_hdf5(paste0(getwd(),"/mcycle_two_layer_mlp_relu.hdf5"))
model
num_MC_samples <- 100
MC_samples <- array(0, dim = c(num_MC_samples, n_val, 2 * 1))
for (k in 1:num_MC_samples) {
MC_samples[k, , ] <- (model %>% predict(x_test))
}
# the means are in the first output column
means <- MC_samples[, , 1:output_dim]
# the means are in the first output column
means <- MC_samples[, , 1:1]
# average over the MC samples
predictive_mean <- apply(means, 2, mean)
epistemic_uncertainty <- apply(means, 2, var)
logvar <- MC_samples[, , (1 + 1):(1 * 2)]
aleatoric_uncertainty <- exp(colMeans(logvar))
df <- data.frame(
x = x_test,
y_pred = predictive_mean,
e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
u_overall_lower = predictive_mean -
sqrt(epistemic_uncertainty) -
sqrt(aleatoric_uncertainty),
u_overall_upper = predictive_mean +
sqrt(epistemic_uncertainty) +
sqrt(aleatoric_uncertainty)
)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)
num_MC_samples <- 100
MC_samples <- array(0, dim = c(num_MC_samples, n_val, 2 * 1))
for (k in 1:num_MC_samples) {
MC_samples[k, , ] <- (model %>% predict(datanorm$times))
}
# the means are in the first output column
means <- MC_samples[, , 1:1]
# average over the MC samples
predictive_mean <- apply(means, 2, mean)
epistemic_uncertainty <- apply(means, 2, var)
logvar <- MC_samples[, , (1 + 1):(1 * 2)]
aleatoric_uncertainty <- exp(colMeans(logvar))
df <- data.frame(
x = x_test,
y_pred = predictive_mean,
e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
u_overall_lower = predictive_mean -
sqrt(epistemic_uncertainty) -
sqrt(aleatoric_uncertainty),
u_overall_upper = predictive_mean +
sqrt(epistemic_uncertainty) +
sqrt(aleatoric_uncertainty)
)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)
num_MC_samples <- 100
MC_samples <- array(0, dim = c(num_MC_samples, n_val, 2 * 1))
for (k in 1:num_MC_samples) {
MC_samples[k, , ] <- (model %>% predict(c(x_train, x_test)))
}
# the means are in the first output column
means <- MC_samples[, , 1:1]
# average over the MC samples
predictive_mean <- apply(means, 2, mean)
epistemic_uncertainty <- apply(means, 2, var)
logvar <- MC_samples[, , (1 + 1):(1 * 2)]
aleatoric_uncertainty <- exp(colMeans(logvar))
df <- data.frame(
x = x_test,
y_pred = predictive_mean,
e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
u_overall_lower = predictive_mean -
sqrt(epistemic_uncertainty) -
sqrt(aleatoric_uncertainty),
u_overall_upper = predictive_mean +
sqrt(epistemic_uncertainty) +
sqrt(aleatoric_uncertainty)
)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)
c(x_train, x_test)
MC_samples <- array(0, dim = c(num_MC_samples, n_val, 2 * 1))
for (k in 1:num_MC_samples) {
MC_samples[k, , ] <- (model %>% predict(x_test))
}
# the means are in the first output column
means <- MC_samples[, , 1:1]
# average over the MC samples
predictive_mean <- apply(means, 2, mean)
epistemic_uncertainty <- apply(means, 2, var)
logvar <- MC_samples[, , (1 + 1):(1 * 2)]
aleatoric_uncertainty <- exp(colMeans(logvar))
df <- data.frame(
x = x_test,
y_pred = predictive_mean,
e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
u_overall_lower = predictive_mean -
sqrt(epistemic_uncertainty) -
sqrt(aleatoric_uncertainty),
u_overall_upper = predictive_mean +
sqrt(epistemic_uncertainty) +
sqrt(aleatoric_uncertainty)
)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)
num_MC_samples <- 100
MC_samples <- array(0, dim = c(num_MC_samples, nrow(datanorm), 2 * 1))
for (k in 1:num_MC_samples) {
MC_samples[k, , ] <- (model %>% predict(datanorm$times))
}
# the means are in the first output column
means <- MC_samples[, , 1:1]
# average over the MC samples
predictive_mean <- apply(means, 2, mean)
epistemic_uncertainty <- apply(means, 2, var)
logvar <- MC_samples[, , (1 + 1):(1 * 2)]
aleatoric_uncertainty <- exp(colMeans(logvar))
df <- data.frame(
x = x_test,
y_pred = predictive_mean,
e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
u_overall_lower = predictive_mean -
sqrt(epistemic_uncertainty) -
sqrt(aleatoric_uncertainty),
u_overall_upper = predictive_mean +
sqrt(epistemic_uncertainty) +
sqrt(aleatoric_uncertainty)
)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)
df <- data.frame(
x = datanorm$times,
y_pred = predictive_mean,
e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
u_overall_lower = predictive_mean -
sqrt(epistemic_uncertainty) -
sqrt(aleatoric_uncertainty),
u_overall_upper = predictive_mean +
sqrt(epistemic_uncertainty) +
sqrt(aleatoric_uncertainty)
)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)
history <- model %>% fit(
x_train, y_train,
batch_size = 16,
epochs = 100,
verbose = 2,
validation_split = 0.1,
callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 100, verbose = 1),
callback_model_checkpoint(paste0(getwd(),"/mcycle_two_layer_concrete_dropout.hdf5"), save_best_only = TRUE))
)
num_MC_samples <- 100
MC_samples <- array(0, dim = c(num_MC_samples, nrow(datanorm), 2 * 1))
for (k in 1:num_MC_samples) {
MC_samples[k, , ] <- (model %>% predict(datanorm$times))
}
# the means are in the first output column
means <- MC_samples[, , 1:1]
# average over the MC samples
predictive_mean <- apply(means, 2, mean)
epistemic_uncertainty <- apply(means, 2, var)
logvar <- MC_samples[, , (1 + 1):(1 * 2)]
aleatoric_uncertainty <- exp(colMeans(logvar))
df <- data.frame(
x = datanorm$times,
y_pred = predictive_mean,
e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
u_overall_lower = predictive_mean -
sqrt(epistemic_uncertainty) -
sqrt(aleatoric_uncertainty),
u_overall_upper = predictive_mean +
sqrt(epistemic_uncertainty) +
sqrt(aleatoric_uncertainty)
)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)
# sample size (training data)
n_train <- nrow(x_train)
# sample size (validation data)
n_val <- nrow(x_test)
# prior length-scale
l <- 1e-5
# initial value for weight regularizer
wd <- l^2/n_train
# initial value for dropout regularizer
dd <- 2/n_train
input <- layer_input(shape = 1)
output <- input %>% layer_concrete_dropout(
layer_dense(units = 64, activation = 'relu', kernel_initializer = 'lecun_normal'),
weight_regularizer = wd,
dropout_regularizer = dd
) %>% layer_concrete_dropout(
layer_dense(units = 32, activation = 'relu', kernel_initializer = 'lecun_normal'),
weight_regularizer = wd,
dropout_regularizer = dd
)
mean <- output %>% layer_concrete_dropout(
layer = layer_dense(units = 1),
weight_regularizer = wd,
dropout_regularizer = dd
)
log_var <- output %>% layer_concrete_dropout(
layer_dense(units = 1),
weight_regularizer = wd,
dropout_regularizer = dd
)
output <- layer_concatenate(list(mean, log_var))
model <- keras_model(input, output)
heteroscedastic_loss <- function(y_true, y_pred) {
mean <- y_pred[, 1:1]
log_var <- y_pred[, (1 + 1):(1 * 2)]
precision <- k_exp(-log_var)
k_sum(precision * (y_true - mean) ^ 2 + log_var, axis = 2)
}
model %>% compile(
optimizer = "adam",
loss = heteroscedastic_loss,
metrics = c(custom_metric("heteroscedastic_loss", heteroscedastic_loss))
)
history <- model %>% fit(
x_train, y_train,
batch_size = 16,
epochs = 100,
verbose = 2,
validation_split = 0.1,
callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 100, verbose = 1),
callback_model_checkpoint(paste0(getwd(),"/mcycle_two_layer_concrete_dropout.hdf5"), save_best_only = TRUE))
)
history <- model %>% fit(
x_train, y_train,
batch_size = 16,
epochs = 100,
verbose = 2,
validation_split = 0.1,
callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 100, verbose = 1),
callback_model_checkpoint(paste0(getwd(),"/mcycle_two_layer_concrete_dropout.hdf5"), save_best_only = TRUE))
)
history <- model %>% fit(
x_train, y_train,
batch_size = 16,
epochs = 100,
verbose = 2,
validation_split = 0.1,
callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 100, verbose = 1),
callback_model_checkpoint(paste0(getwd(),"/mcycle_two_layer_concrete_dropout.hdf5"), save_best_only = TRUE))
)
?compile
model %>% compile(
optimizer = optimizer_adam(lr = 0.001),
loss = heteroscedastic_loss,
metrics = c(custom_metric("heteroscedastic_loss", heteroscedastic_loss))
)
history <- model %>% fit(
x_train, y_train,
batch_size = 16,
epochs = 100,
verbose = 2,
validation_split = 0.1,
callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 100, verbose = 1),
callback_model_checkpoint(paste0(getwd(),"/mcycle_two_layer_concrete_dropout.hdf5"), save_best_only = TRUE))
)
num_MC_samples <- 100
MC_samples <- array(0, dim = c(num_MC_samples, nrow(datanorm), 2 * 1))
for (k in 1:num_MC_samples) {
MC_samples[k, , ] <- (model %>% predict(datanorm$times))
}
# the means are in the first output column
means <- MC_samples[, , 1:1]
# average over the MC samples
predictive_mean <- apply(means, 2, mean)
epistemic_uncertainty <- apply(means, 2, var)
logvar <- MC_samples[, , (1 + 1):(1 * 2)]
aleatoric_uncertainty <- exp(colMeans(logvar))
df <- data.frame(
x = datanorm$times,
y_pred = predictive_mean,
e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
u_overall_lower = predictive_mean -
sqrt(epistemic_uncertainty) -
sqrt(aleatoric_uncertainty),
u_overall_upper = predictive_mean +
sqrt(epistemic_uncertainty) +
sqrt(aleatoric_uncertainty)
)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3) +
geom_point(data = datanorm, aes(x = times, y = accel), col = "red")
2/n_train
model %>% compile(
optimizer = optimizer_adam(lr = 0.01),
loss = heteroscedastic_loss,
metrics = c(custom_metric("heteroscedastic_loss", heteroscedastic_loss))
)
history <- model %>% fit(
x_train, y_train,
batch_size = 64,
epochs = 1000,
verbose = 2,
validation_split = 0.1,
callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 1000, verbose = 1),
callback_model_checkpoint(paste0(getwd(),"/mcycle_two_layer_concrete_dropout.hdf5"), save_best_only = TRUE))
)
model %>% compile(
optimizer = optimizer_adam(lr = 0.0001),
loss = heteroscedastic_loss,
metrics = c(custom_metric("heteroscedastic_loss", heteroscedastic_loss))
)
history <- model %>% fit(
x_train, y_train,
batch_size = 64,
epochs = 1000,
verbose = 2,
validation_split = 0.1,
callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 1000, verbose = 1),
callback_model_checkpoint(paste0(getwd(),"/mcycle_two_layer_concrete_dropout.hdf5"), save_best_only = TRUE))
)
num_MC_samples <- 100
MC_samples <- array(0, dim = c(num_MC_samples, nrow(datanorm), 2 * 1))
for (k in 1:num_MC_samples) {
MC_samples[k, , ] <- (model %>% predict(datanorm$times))
}
# the means are in the first output column
means <- MC_samples[, , 1:1]
# average over the MC samples
predictive_mean <- apply(means, 2, mean)
epistemic_uncertainty <- apply(means, 2, var)
logvar <- MC_samples[, , (1 + 1):(1 * 2)]
aleatoric_uncertainty <- exp(colMeans(logvar))
df <- data.frame(
x = datanorm$times,
y_pred = predictive_mean,
e_u_lower = predictive_mean - sqrt(epistemic_uncertainty),
e_u_upper = predictive_mean + sqrt(epistemic_uncertainty),
a_u_lower = predictive_mean - sqrt(aleatoric_uncertainty),
a_u_upper = predictive_mean + sqrt(aleatoric_uncertainty),
u_overall_lower = predictive_mean -
sqrt(epistemic_uncertainty) -
sqrt(aleatoric_uncertainty),
u_overall_upper = predictive_mean +
sqrt(epistemic_uncertainty) +
sqrt(aleatoric_uncertainty)
)
ggplot(df, aes(x, y_pred)) +
geom_point() +
geom_ribbon(aes(ymin = e_u_lower, ymax = e_u_upper), alpha = 0.3) +
geom_point(data = datanorm, aes(x = times, y = accel), col = "red")
